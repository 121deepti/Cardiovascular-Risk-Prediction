{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "ArJBuiUVfxKd",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "bmKjuQ-FpsJ3",
        "7AN1z2sKpx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/121deepti/Cardiovascular_Risk_Prediction/blob/main/Cardiovascular_Risk_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to predict the 10-year risk of future coronary heart disease (CHD) for patients in Framingham, Massachusetts. A dataset containing demographic, behavioral, and medical risk factors for over 4000 patients is used to build a predictive model. The model will use machine learning techniques to analyze the provided information and make accurate CHD risk predictions. The goal of the project is to develop a tool for early detection and prevention of CHD, addressing a significant public health concern. The outcome of the project will be a predictive model that can be used by healthcare providers to make informed decisions regarding patient care.\n",
        "\n",
        "There were approximately 3390 records and 17 attributes in the dataset.\n",
        "We started by importing the dataset, and necessary libraries and conducted exploratory data analysis (EDA).\n",
        "Outliers and null values were removed from the raw data and treated. Data were transformed to ensure that it was compatible with machine learning models.\n",
        "We handled target class imbalance using SMOTE.\n",
        "Then finally cleaned and scaled data was sent to 8 various models, the metrics were made to evaluate the model, and we tuned the hyperparameters to make sure the right parameters were being passed to the model.\n",
        "When developing a machine learning model, it is generally recommended to track multiple metrics because each one highlights distinct aspects of model performance. We are, focusing more on the Recall score and F1 score.\n",
        "It is categorically unacceptable to miss identifying a particular patient or to classify a particular patient as healthy (false negative). That is why we have preferred recall score."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/121deepti/Cardiovascular_Risk_Prediction/blob/main/Cardiovascular_Risk_Prediction.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What exactly are cardiovascular diseases?**\n",
        "\n",
        "A group of conditions affecting the heart and blood vessels is known as cardiovascular diseases. They consist of heart disease, which affects the blood vessels that supply the heart muscle. The majority of the time, a blockage that prevents blood from flowing to the heart or brain is to blame for heart attacks and strokes, which are typically sudden events. A buildup of fatty deposits on the inner walls of the blood vessels that supply the heart or brain is the most common cause of this.\n",
        "\n",
        "The goal of the classification is to predict the 10-year risk of future coronary heart disease (CHD) for patients. The issue of coronary heart disease is a significant public health concern and early prediction of CHD risk is crucial for preventative measures. The dataset is from an ongoing cardiovascular study on residents of Flamingham, Massachusetts. The data set includes over 4000 records and 16 attributes, each of which is a potential risk factor, including demographic, behavioral, and medical risk factors.\n",
        "\n",
        "**WHY DO WE NEED CARDIOVASCULAR RISK PREDICTION?**\n",
        "\n",
        "The greatest obstacle facing the medical industry is accurately predicting and diagnosing heart disease. Heart diseases are influenced by numerous factors.\n",
        "Heart disease is even referred to as a \"silent killer\" because it kills people without showing any obvious symptoms.\n",
        "When high-risk patients are diagnosed with heart disease early, it is easier to make lifestyle changes, which in turn lowers the risk of complications.\n",
        "Based on the way people currently live, machine learning can help predict the likelihood of heart disease in the coming years."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import  make_scorer,f1_score,roc_curve,accuracy_score,classification_report,confusion_matrix,roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imblearn.combine import SMOTETomek"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AFSKXkw26Fy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/my_data/data_cardiovascular_risk.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Missing values percentage\n",
        "round((df.isna().sum().sort_values(ascending=False))*100/df.shape[0],2)"
      ],
      "metadata": {
        "id": "ZLvDxIZ29DJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-profiling"
      ],
      "metadata": {
        "id": "4EPfVxWR_0Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
        "profile"
      ],
      "metadata": {
        "id": "3fAyRlYdwAeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3390 rows and 17 columns in the dataset. No duplicates are found in the dataset.Some Null values are observed in the features."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset provides the patients’ information. It includes over 3,390 records and 17 attributes(from which TenYearCD is the target column). Variables Each attribute is a potential risk factor. There are demographic, behavioural, and medical risk factors\n",
        "\n",
        "Demographic:\n",
        "*   Sex: male or female (\"M\" or \"F\")\n",
        "*   Age: Age of the patient (Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "*   Education: The level of education of the patient (categorical values - 1,2,\n",
        "3,4)\n",
        "\n",
        "Behavioral:<br>\n",
        "*   is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "*   Cigs Per Day: the number of cigarettes that the person smoked on average in one day\n",
        "\n",
        "Medical(History):\n",
        "\n",
        "\n",
        "*   BP Meds: whether or not the patient was on blood pressure medication\n",
        "*   Prevalent Stroke: whether or not the patient had previously had a stroke\n",
        "*   Prevalent Hyp: whether or not the patient was hypertensive\n",
        "*   Diabetes: whether or not the patient had diabetes (Nominal) Medical(current)\n",
        "*   Tot Chol: total cholesterol level\n",
        "*   Sys BP: systolic blood pressure\n",
        "*   Dia BP: diastolic blood pressure\n",
        "*   BMI: Body Mass Index\n",
        "*   Heart Rate: heart rate\n",
        "*   Glucose: glucose level\n",
        "*   TenYearCHD(**Target Variable**): 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns:\n",
        "  print(\"No of Unique Values in \", i, \" is:\", df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a copy of dataset\n",
        "df_eda=df.copy()"
      ],
      "metadata": {
        "id": "wuJMctAK-w9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevalent stroke effect on heart disease\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.prevalentStroke,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "7ly3THlQ2GFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevalent Hypertension impact on heart disease\n",
        "print(pd.crosstab(df_eda.prevalentHyp,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.prevalentHyp,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "7noclZVw3HKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating age bins and impact on heart disease\n",
        "df_eda['age_bins'] = pd.cut(x=df_eda['age'], bins=[30, 35, 40, 45,50,55,60,65,70])\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.age_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "6c3_E6kL56Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating BMI bins and finding its relation with heart disease\n",
        "df_eda['BMI_bins'] = pd.cut(x=df_eda['BMI'], bins=[15,25,35,45,55,65])\n",
        "print(pd.crosstab(df_eda.BMI_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.BMI_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "TzbD843vG2_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating cigrette bins and finding its relation with heart disease\n",
        "df_eda['cig_bins'] = pd.cut(x=df_eda['cigsPerDay'], bins=[10, 20,30,40,50,60])\n",
        "\n",
        "print(pd.crosstab(df_eda.cig_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.cig_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "wL6n1i-KI-nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Relation of Sex with heart disease\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.sex,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "3vGrwHxBFfGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diebetes impact on heart disease\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.diabetes,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "RJ6x-JORAHwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BP medication impact on heart disease\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.BPMeds,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "H4rYPoADE2ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating cholestrol bins and finding its relation with heart disease\n",
        "df_eda['chol_bins'] = pd.cut(x=df_eda['totChol'], bins=[100, 200,300,400,500,600,700])\n",
        "print(pd.crosstab(df_eda.chol_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.chol_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "ndujkExlL4qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating heartRate bins and finding its relation with heart disease\n",
        "df_eda['heartRate_bins'] = pd.cut(x=df_eda['heartRate'], bins=[40,60,80,100,120,140,160])\n",
        "print(pd.crosstab(df_eda.heartRate_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.heartRate_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "t_MxWGENPMnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Glucose bins and finding its relation with heart disease\n",
        "df_eda['Glucose_bins'] = pd.cut(x=df_eda['glucose'], bins=[40,150,250,350,450])\n",
        "# We can check the frequency of each bin\n",
        "print(pd.crosstab(df_eda.Glucose_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.Glucose_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "rTVndYhoQBgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statics about glucose when the person is diabetic\n",
        "df_eda.loc[df_eda.diabetes==1]['glucose'].describe()"
      ],
      "metadata": {
        "id": "ZvxrZ4p-O7Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Statics about glucose when the person is non-diabetic\n",
        "df_eda.loc[df_eda.diabetes==0]['glucose'].describe()"
      ],
      "metadata": {
        "id": "6GdmC9t3iVp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Number of patients at heart risk and safe\n",
        "print(len(df_eda[df_eda['TenYearCHD']==1]))\n",
        "print(len(df_eda[df_eda['TenYearCHD']==0]))"
      ],
      "metadata": {
        "id": "Fmzb_XZYROqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#smoking impact on heart disease\n",
        "print(pd.crosstab(df_eda.is_smoking,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "print(\"Analysis in %age form\\n\")\n",
        "pd.crosstab(df_eda.is_smoking,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "nvfqC26bKq-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating seperate list of numeric and categorical columns\n",
        "numeric_cols=['age','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
        "cate_cols=['sex','education','is_smoking','BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes','TenYearCHD']"
      ],
      "metadata": {
        "id": "hs6y1Zwy4nnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are some important insights-\n",
        "*   The persons who have stroke history has a strong chance of heart disease.\n",
        "*   Hyper tension patients has more chance of heart disease.\n",
        "*  I have created bins for age group and found that age has strong correlation with heart disease i.e. as the age increases the chances of heart disease increases too.\n",
        "*   I have created a column cigeratte bins and found that a large number of people is using 10-20 cigrette in a day. As their number of cigrettes increased the chances of heart disease also increased.\n",
        "* There are more males are suffered from heart disease.  \n",
        "*  A diabetic and high BP and high blood sugar(glucose) patient having strong chances of heart disease .\n",
        "*  There are a lot of persons belonging to 200-300 cholestrol range. As the cholestrol level goes up the chances of heart disease also goes up .\n",
        "*  Increased heart rate increases the risk of heart diseases.\n",
        "*  The patients at risk ~  500 but in safe circle ~ 2900 so the problem is totally unbalanced\n",
        "*   The smokers having high chance of heart disease. According to research,About 20% of deaths from heart disease in the U.S. are directly related to smoking."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Univariate Analysis**"
      ],
      "metadata": {
        "id": "X1kE-j4oR7SM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 Count plot of Categorical columns"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(cate_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.countplot(x =item, data = df,hue='TenYearCHD')\n",
        "  for p in ax.patches:\n",
        "    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "  plt.tight_layout()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps in analyzing the categorical features count in the data set."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The findings are:\n",
        "*   There are more number of females as compared to male.\n",
        "*   Almost equal ratio between smoker and non-smoker.\n",
        "*   There are more number of non-BP patients as compared to BP patients.\n",
        "*   The persons who history of prevalent stroke are very less.\n",
        "*   Hypertension and Diabetic patients are less in number.\n",
        "*   There are more number of patients who are in less risk zone."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 Pie charts for Categorical Columns"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for index,item in enumerate(cate_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  df[item].value_counts().plot(kind='pie',autopct='%1.0f%%')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the percentage distribution of categorical data"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratio is as follows:\n",
        "*   57% female and 43% males\n",
        "*   50% smokers and 50% non-smokers\n",
        "*   97% NonBP and NonDiebitic patients while 3% BP and Diebitic patients\n",
        "*   99% have No Stroke history\n",
        "*   68% non-Hypertension Patients  \n",
        "*   85% are under safe category\n",
        "*   42% people belong to education category 1 and category 4 has lowest 11% people."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3  Histogram for Numerical columns"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(numeric_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.distplot(df[item])\n",
        "  sk=round(df[item].skew(),2)\n",
        "  ax=fig.gca()\n",
        "  ax.axvline(df[item].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(df[item].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(item+'  skewness'+str(sk))\n",
        "  plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart displays the PDF of numerical variables with their skewness."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some important findings-\n",
        "*   Except cigs_per_day all other are following Normal or almost Normal Distribution\n",
        "*   All features are Positively skewed except age but cigs_per_day and BMI highly skewed but Glusose is very high skewed ~6.14."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 Log Transformation of skewed variables"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(numeric_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  if item=='age':\n",
        "    sns.distplot(df[item])\n",
        "    sk=round(df[item].skew(),2)\n",
        "    ax=fig.gca()\n",
        "    ax.axvline(df[item].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(df[item].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  else:\n",
        "    sns.distplot(np.log1p(df[item]))\n",
        "    sk=np.log1p(round(df[item].skew(),2))\n",
        "    ax=fig.gca()\n",
        "    ax.axvline(np.log1p(df[item]).mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(np.log1p(df[item]).median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(item+'  skewness'+str(sk))\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps to show the variable distribution after log transformation."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying log transformation, BMI skewness is reduced to ~ 0.7,Cigs perday to ~0.8 and Glucose to ~1.97."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 Box plot Analysis"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "for index,item in enumerate(numeric_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.boxplot(x =item, data = df)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart analyses the outliers present in numerical features."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the outcomes of this chart-\n",
        "*   \"Age\"-No outlier is present\n",
        "*   \"cigs Per Day\" -Two outliers are present beyond the upper limit.\n",
        "*   \"totChol\", \"diaBP\",\"heartRate\" and \"glucose\"-Many outliers are present beyond the upper limit and some are present on the lower boundary too.\n",
        "*   \"SysBP\" and \"BMI\"-Many outliers are present beyond the upper boundary."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers can mislead the analysis of data that can give wrong signals to the business."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Bivariate Analysis**"
      ],
      "metadata": {
        "id": "qnk1IpK_B2c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6 Relation of SystolicBP and diastolicBP"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6\n",
        "print (numeric_cols)\n",
        "print(cate_cols)\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=df,x='sysBP',y='diaBP')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the relationship between sysBP and diaBP."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is linear relationship between sysBP and diaBP."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sysBP and diaBP are crucial factor in determining the risk of heart disease, so they should be in controllled manner."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7 Bar graphs for categorical vs numerical features analysis"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7\n",
        "index=1\n",
        "fig = plt.figure(figsize=(12,10))\n",
        "for x in cate_cols[0:-1]:\n",
        "  for y in numeric_cols:\n",
        "    plt.subplot(7,8,index)\n",
        "    ax = fig.gca()\n",
        "    sns.barplot(data=df,x=x,y=y)\n",
        "    index+=1\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart reflects the relationship between numerical and categorical variables."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are some of the takeaway from the above analysis:\n",
        "*   According to Sex, Males are more smokers than females.\n",
        "*   Education 1.0 has high ratio of age,BMI and glucose level. education level 2.0 has more smokers.\n",
        "*  There are high number of young smokers. They have low statics(positive) as compared to old age smokers but their high rate is higher.\n",
        "*   The persons on BP medication are old age persons and having higher statics(negative) as compared to young man but they are less smokers comparatively.\n",
        "*   The persons having stroke history are older and having higher BP,glucose and BMI but cigeratte consuption and heart rate is lower comparatively.\n",
        "*   HyperTension and diabetic patients are the older one having all the statics on higher(negative)side except smoking.\n",
        "    <p>In short, age is a crucial factor in determining risk. Young age are more involved in smoking but due to their age they are in safe zone.\n",
        "    According to studies,an older heart rests at a lower heart rate compared to when you were younger thats why this pattern is observed in the dataset"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis helps in recognizing the crucial factors that have strong infulence in determing heart disease."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8 Boxplot analysis of categorical variable w.r.t to numerical variable"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8\n",
        "fig = plt.figure(figsize=(10, 12))\n",
        "plt.subplot(5,4,1)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,x='cigsPerDay',y='is_smoking')\n",
        "\n",
        "plt.subplot(5,4,2)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,y='glucose',x='diabetes')\n",
        "\n",
        "plt.subplot(5,4,3)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,y='sysBP',x='BPMeds')\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+4)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='sysBP',x=item)\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes','TenYearCHD','BPMeds','sex']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+7)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='BMI',x=item)\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes','TenYearCHD']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+13)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='heartRate',x=item)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps in understanding the presence of outliers in various sceniors."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the insights as per my undersatnding\n",
        "*   Some persons are smoking beyond the limit.\n",
        "*   In case of Non-Diebitic persons lot of outliers are present in their glucose level.\n",
        "*   Persons not on BP medication, no stroke history and non-diebitic having a lot high range of outliers for sysBP are present but high range of outliers are seen in prevalent as well as non prevalent hypertension patients.\n",
        "*   In BMI case, high range of outliers are present in high risk and low risk patients.BP medicated ,Males having lesser outliers comparatively.No stroke history patients,Non Diebitic having higher outliers.For HyperTension or not BMI showing high range of outliers present.\n",
        "*   In HeartRate case,Non prevalent stroke,Non diabetic having higher range of outliers,For HyperTension or not Heart Rate showing high range of outliers present and same in case of risk factor."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persons not on BP medications,not Diebitic and no stroke history having outliers so these must be inspected carefully otherwise can lead to wrong prediction."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9 Relation between independent variables with the target varibale(TenYearCHD)"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9\n",
        "index=1\n",
        "fig = plt.figure(figsize=(12,4))\n",
        "x='TenYearCHD'\n",
        "for y in numeric_cols:\n",
        "    plt.subplot(2,8,index)\n",
        "    ax = fig.gca()\n",
        "    sns.barplot(data=df,x=x,y=y)\n",
        "    index+=1\n",
        "for y in cate_cols[0:-1]:\n",
        "  plt.subplot(2,8,index)\n",
        "  ax=fig.gca()\n",
        "  pd.crosstab(df[y],df[x]).plot(kind='bar',ax=ax)\n",
        "  index+=1\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows the relation of independent variable impact on dependent variable."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps in deciding the factors that influence the target variable like age,cigeratte per day,cholestrol,BMI and glucose.\n",
        "Gender,smoking,BP,diabetes,hypertension also plays crucial role in deciding the future heart disease patients."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart focus on feature importance section which helps in correct predictions."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10 Correlation Heatmap"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11\n",
        "plt.figure(figsize=(12,12))\n",
        "correlation =df.corr()\n",
        "sns.heatmap(abs(correlation), annot=True)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is important to know the correlation between various dependent and independent features."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some important insights are-\n",
        "*   sysBP and DiaBP is highly correlated with prevalent Hypertension and BMI.\n",
        "*   Glucose is highly correlated with diabetes.\n",
        "*   sysBP and diaBP is highly correlated.\n",
        "*   age is correlated with prevalent hypertension,Choloestrol ,sysBP and diaBP as well.\n",
        "*   BP meds is correlated with sysBP,diaBP,hypertension\n",
        "*   TenYearCHD is correlated with age,sysBP,diaBP,glucose and hypertension."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11 Pair Plot"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df,hue='TenYearCHD')"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "4ZEpI2IZLHwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart I got to know, there are less linear relationship between variables and data points aren't linearly separable.Only sysBP and diaBP shows linear relationship.Not at risk patients data is clusetered and ovearlapped each other but risky patients data is in scattered form.There are less number of  risky patients"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. HyperTension patients who are at risk is at least 200.\n",
        "2. Diabetic patients who are at rick is at most 20.\n",
        "3. Average age of non risky patients is 50."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from scipy.stats import *\n",
        "\n",
        "# Creating Parameter Class\n",
        "class findz:\n",
        "  def proportion(self,sample,hyp,size):\n",
        "    return (sample - hyp)/math.sqrt(hyp*(1-hyp)/size)\n",
        "  def mean(self,hyp,sample,size,std):\n",
        "    return (sample - hyp)*math.sqrt(size)/std\n",
        "  def varience(self,hyp,sample,size):\n",
        "    return (size-1)*sample/hyp\n",
        "\n",
        "variance = lambda x : sum([(i - np.mean(x))**2 for i in x])/(len(x)-1)\n",
        "zcdf = lambda x: norm(0,1).cdf(x)\n",
        "# Creating a function for getting P value\n",
        "def p_value(z,tailed,t,hypothesis_number,df,col):\n",
        "  if t!=\"true\":\n",
        "    z=zcdf(z)\n",
        "    if tailed=='l':\n",
        "      return z\n",
        "    elif tailed == 'r':\n",
        "      return 1-z\n",
        "    elif tailed == 'd':\n",
        "      if z>0.5:\n",
        "        return 2*(1-z)\n",
        "      else:\n",
        "        return 2*z\n",
        "    else:\n",
        "      return np.nan\n",
        "  else:\n",
        "    z,p_value=stats.ttest_1samp(df[col],hypothesis_number)\n",
        "    return p_value\n",
        "\n",
        "# Conclusion about the P - Value\n",
        "def conclusion(p):\n",
        "  significance_level = 0.05\n",
        "  if p>significance_level:\n",
        "    return f\"Failed to reject the Null Hypothesis for p = {p}.\"\n",
        "  else:\n",
        "    return f\"Null Hypothesis rejected Successfully for p = {p}\"\n",
        "\n",
        "# Initializing the class\n",
        "findz = findz()\n"
      ],
      "metadata": {
        "id": "RjJMRdnAQeXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1\n",
        "The persons at risk having sysBP at least 120."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N = 120\n",
        "\n",
        "Alternate Hypothesis : N < 120\n",
        "\n",
        "Test Type: Left Tailed Test"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "KoHj_an1XpJp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['TenYearCHD']==0]['diaBP'].mean()"
      ],
      "metadata": {
        "id": "juVm0vzZYAVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypo_1=df[(df[\"TenYearCHD\"]==1)]\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 120\n",
        "sample_mean = hypo_1[\"sysBP\"].mean()\n",
        "size = len(hypo_1)\n",
        "std=(variance(hypo_1[\"sysBP\"]))**0.5"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean,size,std)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='l',t=\"true\",hypothesis_number=hypothesis_number,df=hypo_1,col=\"sysBP\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "4hp9azGmXrcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found that we failed to reject the null hypothesis and there is atleast 200 hypertension patients who are at risk."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (hypo_1[\"sysBP\"])\n",
        "sns.distplot(hypo_1[\"sysBP\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2vKRq0UIWlBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean is approximately same as the median. Thus, it is almost a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2\n",
        "The persons who are not at risk having diaBP is atmost 80"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N = 20\n",
        "\n",
        "Alternate Hypothesis : N > 80\n",
        "\n",
        "Test Type: Right Tailed Test"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypo_2=df[(df[\"TenYearCHD\"]==0)]\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 80\n",
        "sample_mean = int(hypo_2[\"diaBP\"].mean())\n",
        "size = len(hypo_2)\n",
        "std=(variance(hypo_2[\"diaBP\"]))**0.5"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean,size,std)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='r',t=\"false\",hypothesis_number=hypothesis_number,df=hypo_2,col=\"diaBP\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "y7KwmjOAb5VQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found that we reject the null hypothesis and diaBP is more than 80 who are not at risk."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (hypo_2[\"diaBP\"])\n",
        "sns.distplot(hypo_2[\"diaBP\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Hh5BdKAKbtO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean is approximately same as the median. Thus, it is almost a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3\n",
        "Average age of non risky patients is 50."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis: N = 50\n",
        "\n",
        "Alternate Hypothesis : N != 50\n",
        "\n",
        "Test Type: Two Tailed Test\n",
        "\n"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['TenYearCHD']==0]['age'].mean()"
      ],
      "metadata": {
        "id": "kELUPzUkLvZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypo_3=df[(df[\"TenYearCHD\"]==0)]\n",
        "# Getting the required parameter values for hypothesis testing\n",
        "hypothesis_number = 50\n",
        "sample_mean = round(hypo_3[\"age\"].mean())\n",
        "print(sample_mean)\n",
        "size = len(hypo_3)\n",
        "print(size)\n",
        "std=(variance(hypo_3[\"age\"]))**0.5\n",
        "print(std)"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting Z value\n",
        "z = findz.mean(hypothesis_number,sample_mean,size,std)\n",
        "# Getting P - Value\n",
        "p = p_value(z=z,tailed='d',t=\"false\",hypothesis_number=hypothesis_number,df=hypo_3,col=\"age\")\n",
        "# Getting Conclusion\n",
        "print(conclusion(p))"
      ],
      "metadata": {
        "id": "dEfRW2JPdzpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Z-Test as the statistical testing to obtain P-Value and found that we reject the null hypothesis and say that non risky patients avg height is not 50."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "fig=plt.figure(figsize=(9,6))\n",
        "ax=fig.gca()\n",
        "feature= (hypo_3[\"age\"])\n",
        "sns.distplot(hypo_3[\"age\"])\n",
        "ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1TmuwFCFoocz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As shown in the figure the mean is approximately same as the median. Thus, it is almost a Normal Distribution. That's why I have used Z-Test directly."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy of data frame\n",
        "df_cpy=df.copy()"
      ],
      "metadata": {
        "id": "f8S_uvzEgm8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "y2PfxN7nqP4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# features which has less than 5%  null values present.\n",
        "nan_columns = ['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate']\n",
        "\n",
        "# dropping null values\n",
        "df_cpy.dropna(subset=nan_columns, inplace=True)\n",
        "\n",
        "#glucose has ~8% null values\n",
        "df_cpy['glucose'] = df_cpy.glucose.fillna(df_cpy.glucose.median())\n",
        "\n",
        "df_cpy.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpy.shape"
      ],
      "metadata": {
        "id": "QnexKWrRqHDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, we use other records to replace these null values. However, the entries in this dataset are person-specific. The values vary from person to person, and the dataset is related to the medical field in this particular instance. Consequently, removing rows with any null value is the most logical choice we have for dealing with such values.\n",
        "\n",
        "We cannot take any risks with this prediction, so if we attempt to impute null values using advanced methods, it may affect the outcome because the values will be incorrect.\n",
        "\n",
        "In the healthcare industry, every piece of data is crucial. Because of this, we came up with a solution by setting a threshold value. If a feature has less than 5% null values, we decide to drop those rows, and the remaining rows are imputing, which will affect prediction but not significantly.As in case of glucose ~8% null values and outliers are present so we replace it with median value.Finally there are 3189 rows and 17 columns.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpy.describe().T"
      ],
      "metadata": {
        "id": "0JpkS1iLtsGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen in the statistical summary for numerical features, there is a significant difference between the 75% percentile and maximum value, indicating that the dataset contains skewness and outliers."
      ],
      "metadata": {
        "id": "jEgdjP4llIWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(10,5))\n",
        "# boxplot of numerical features\n",
        "sns.boxplot(data=df_cpy[numeric_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EayF3mbllHfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As lot of outliers are present since we have limited datapoint hence we are not simply removing the outlier instead of that we are using the clipping method."
      ],
      "metadata": {
        "id": "lkBUmSoil_LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def clip_outliers(risk_df):\n",
        "    for col in risk_df[numeric_cols]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = risk_df[col].quantile(0.25)\n",
        "        q3 = risk_df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # replacing the outliers with upper and lower bound\n",
        "        risk_df[col] = risk_df[col].clip(lower_bound, upper_bound)\n",
        "    return risk_df"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the function to treat outliers\n",
        "df_cpy = clip_outliers(df_cpy)"
      ],
      "metadata": {
        "id": "NFhlr7odm_7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BoxPlot after clipping outliers\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df_cpy[numeric_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPrSr4Qknzwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented **clipping method**.  In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range.\n",
        "Here we have set the threshold of .25-1.5*IQR(Lower limit) and .75+1.5*IQR(Upper limit)."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df_cpy['sex']=df_cpy['sex'].map({'M':1,'F':0})\n",
        "df_cpy['is_smoking']=df_cpy['is_smoking'].map({'YES':1,'NO':0})"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encode the 'education' feature\n",
        "education_onehot = pd.get_dummies(df_cpy['education'], prefix='education',drop_first=True)\n",
        "\n",
        "# drop the original education feature\n",
        "df_cpy.drop('education', axis=1, inplace=True)\n",
        "\n",
        "# concatenate the one-hot encoded education feature with the rest of the data\n",
        "df_cpy = pd.concat([df_cpy, education_onehot], axis=1)\n",
        "df_cpy.head(3)"
      ],
      "metadata": {
        "id": "7RzHZpodu1AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpy.shape"
      ],
      "metadata": {
        "id": "gcxWGsQqp4M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes'and 'TenYearCHD' are categorical type of features but already have numeric values.We have encoded the \"sex\" and \"is_smoking\" columns to number.As \"education\" column has 4 unique values ,we converted it to object data type and perform one hot encoding to it.Now there is 19 columns in the data set.  "
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)<BR>\n",
        "**As in dataset we have no textual data so we have skipped this step.**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVkZv3D5y-Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#create a new feature MAP(Mean Arterial Pressure) by using sysBP and diaBP\n",
        "df_cpy['Pulse_Pressure']=df_cpy['sysBP']-df_cpy['diaBP']\n",
        "#Dropping sysBP and DiaBP columns\n",
        "df_cpy.drop(['sysBP','diaBP'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking data, weather the provide information is correct or not\n",
        "df_cpy[(df_cpy.is_smoking == 1) & (df_cpy.cigsPerDay == 0)]"
      ],
      "metadata": {
        "id": "HuQBDZCpKdOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# droping is_smoking column due to multi-collinearity\n",
        "df_cpy.drop('is_smoking', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "KYhUHMaER1Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping ID column as not relevant\n",
        "df_cpy.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "fWjdgutNpGT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpy.shape"
      ],
      "metadata": {
        "id": "-CLfIn9Eqzw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now columns are reduced to 16."
      ],
      "metadata": {
        "id": "D9yRiG88q5w0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#updating the numeric and categorical columns list\n",
        "numeric_cols.remove('sysBP')\n",
        "numeric_cols.remove('diaBP')\n",
        "cate_cols.remove('is_smoking')\n",
        "numeric_cols.append('Pulse_Pressure')"
      ],
      "metadata": {
        "id": "ppyl9z4yqKOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting correlation heatmap to check multicollinearity.\n",
        "plt.figure(figsize=(15,4))\n",
        "sns.heatmap(df_cpy.corr(),annot=True)"
      ],
      "metadata": {
        "id": "dxOK4Jz1tRnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VIF\n",
        "def calc_vif(X):\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "   return(vif)"
      ],
      "metadata": {
        "id": "Cbi-wpSsNdXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking VIF of all the columns after excluding high VIF columns\n",
        "calc_vif(df_cpy[[i for i in df_cpy.describe().columns  if i not in['glucose','BMI','heartRate','totChol','Pulse_Pressure']]])"
      ],
      "metadata": {
        "id": "6gwIeKY5R7s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing high VIF columns and create a new data frame named df_removed\n",
        "df_removed=df_cpy.drop(['Pulse_Pressure','glucose','BMI','totChol','heartRate'],axis=1)"
      ],
      "metadata": {
        "id": "uDvFOBXISnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating the numeric column list\n",
        "del numeric_cols[2:]\n",
        "numeric_cols"
      ],
      "metadata": {
        "id": "eNnLLf9ZzMv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.shape"
      ],
      "metadata": {
        "id": "uw2BCT396i-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we have checked the VIF of all features and remove the features which are having high VIF(less than 10) and less important wrt. target variable.\n",
        "    This step is necessary as it saves our model from overfitting by removing multicollnerity."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After removing collinear features the data frame is left with 11 features which are important for building the model.From which \"age\" is the most important feature highly correlated with the target variable.  "
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in numeric_cols:\n",
        "  fig=plt.figure(figsize=(4,5))\n",
        "  ax=fig.gca()\n",
        "  feature= (df_removed[col])\n",
        "  sns.distplot(df_removed[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col+' '+str(feature.skew()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed['cigsPerDay']=np.log1p(df_removed['cigsPerDay'])\n",
        "fig=plt.figure(figsize=(4,5))\n",
        "ax=fig.gca()\n",
        "sns.distplot(df_removed['cigsPerDay'])\n",
        "ax.axvline(df_removed['cigsPerDay'].mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(df_removed['cigsPerDay'].median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('cigsPerDay'+' '+str(df_removed['cigsPerDay'].skew()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KAMXK_dqLS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the numeric features after outliers removal are almost following Gaussian Distribution and having skewness less than 0.5 which seems quite normal.\n",
        "  But \"cigsPerDay\" having skewness >1 so i have applied log tranforamtion to make it follow Gaussian Distribution and finally its skewness is reduced less than 0.5"
      ],
      "metadata": {
        "id": "kpcRHbyKyPFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Created X and y dataset\n",
        "#creating X(independent features) and y(target feature)\n",
        "X_cols=df_removed.copy()\n",
        "y=df_removed.TenYearCHD\n",
        "X_cols.drop('TenYearCHD',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "mmhK5FRxuQgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "X=scalar.fit_transform(X_cols)\n",
        "y=y"
      ],
      "metadata": {
        "id": "H17v1ydL0e17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Standard Scaler to scale the indendepent features as all the numeric features are following Gaussian Distribution."
      ],
      "metadata": {
        "id": "1ryQ9N0CJ5Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per my knowledge, for this dataset dimensionality reduction is not required.\n",
        "\n",
        "Essentially where high dimensions are a problem or where it is a particular point in the algorithm to dimension reduction.\n",
        "\n",
        "Hard rules are hard to state, other than “after you have tried it, did it improve matters”, which isn’t always the most useful guidance.\n",
        "\n",
        "Instead, looking at why we might want to do this we can get a bit of insight. Admittedly some of the following might blur together a bit at the edges but the aim is to give a flavour.\n",
        "\n",
        "1. Our data are too big. 4 million rows. 50,000 columns… is there a lot of redundancy there? Building a model on this could be very expensive. Even relatively simple dimension reduction techniques like PCA can capture almost all of the information in a fraction of the memory if there are strong relationships (that can be linearly approximated) in the data.\n",
        "\n",
        "2. We are over-fitting. If you build a model with tens of thousands of degrees of freedom but don’t have a lot of examples you can easily overfit. Dimension reduction is one way of handling this, though often not the the best\n",
        "\n",
        "3. We want to bring in external data. OK, this is a bit different but worth a note. In applications like word2vec we want to build a classifier using an embedding. We may want to classify some text into different categories but with only a limited number of examples. The complexity of free text is vast but a low dimension embedding is much smaller and will not overfit so badly in a classifier. Building a low dimensional embedding on external text, applying it to the text to be classified then building a classifier is using dimension reduction to bring in external data.\n",
        "\n",
        "4. We suffer from the curse of dimesnionality. Consider something like a nearest neighbour search. As the number of dimensions gets large we see some unwanted behaviour, especially if we are looking at things like euclidean distances. Projecting your data to a lower dimensional space for nearest neighbour, clustering or outlier detection can be both more robust and more meaningful.\n",
        "\n",
        "5. Some tools are all about this. Collaborative filtering through matrix factorisation is an example. Can we approximately describe behaviour as a linear combination of a smaller number of preferences/behaviours?"
      ],
      "metadata": {
        "id": "greCxziR1TgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# split into 70:30 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0,stratify=y)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
        "\n",
        "You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio.\n",
        "\n",
        "In this case the training dataset is small, that's why I have taken 70:30 ratio.\n",
        "As my target variable is highly imbalanced i have used Stratified Sampling so that training and testing set get equal proportion of 1 and 0 class."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(df_removed.TenYearCHD.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "df_removed['TenYearCHD'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Not at Risk(%)','at Risk(%)'],\n",
        "                               colors=['skyblue','red'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "AFHLSMeA2bKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here one can easily notice that ~85% persons are safe only ~15% are at risk, so the ratio is 85:15 which is the sign of unbalanced dataset."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Handling class imbalance by oversampling followed by removing the Tomek link\n",
        "X_smote, y_smote = SMOTETomek(random_state=42).fit_resample(X_train, y_train)\n",
        "# Checking Value counts for both classes Before and After handling Class Imbalance:\n",
        "for col,label in [[y_train,\"Before\"],[y_smote,'After']]:\n",
        "  print(label+' Handling Class Imbalace:')\n",
        "  print(col.value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) followed by removing the Tomek link for balanced the 85:15 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE.\n",
        "\n",
        "To address this disparity, balancing schemes that augment the data to make it more balanced before training the classifier were proposed. Oversampling the minority class by duplicating minority samples or undersampling the majority class is the simplest balancing method.\n",
        "\n",
        "The idea of incorporating synthetic minority samples into tabular data was first proposed in SMOTE, where synthetic minority samples are generated by interpolating pairs of original minority points.\n",
        "\n",
        "SMOTE is a data augmentation algorithm that creates synthetic data points from raw data. SMOTE can be thought of as a more sophisticated version of oversampling or a specific data augmentation algorithm.\n",
        "\n",
        "SMOTE has the advantage of not creating duplicate data points, but rather synthetic data points that differ slightly from the original data points. SMOTE is a superior oversampling option.\n",
        "\n",
        "That's why for lots of advantages, I have used SMOTE technique for balancing the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train the input model and print evaluation matrix\n",
        "def analyse_model(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "  '''Takes classifier model and train test splits as input and prints the\n",
        "  evaluation matrices with the plot and returns the model'''\n",
        "  # Feature importances\n",
        "  try:\n",
        "    try:\n",
        "      importance = model.feature_importances_\n",
        "      feature = X_cols.columns\n",
        "    except:\n",
        "      importance = np.abs(model.coef_[0])\n",
        "      feature = X_cols.columns\n",
        "    indices = np.argsort(importance)\n",
        "    indices = indices[::-1]\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Plotting Evaluation Metrics for train and test dataset\n",
        "  for x, act, label in ((X_train, y_train, 'Train-Set'),(X_test, y_test, \"Test-Set\")):\n",
        "\n",
        "    # Getting required metrics\n",
        "    pred = model.predict(x)\n",
        "    if isinstance(model, SVC):\n",
        "      report = pd.DataFrame(classification_report(y_true=act,y_pred=pred, output_dict=True))\n",
        "    else:\n",
        "      pred_proba = model.predict_proba(x)[:,1]\n",
        "      report = pd.DataFrame(classification_report(y_true=act,y_pred=pred, output_dict=True))\n",
        "      fpr, tpr, thresholds = roc_curve(act, pred_proba)\n",
        "\n",
        "\n",
        "    # Classification report\n",
        "    plt.figure(figsize=(18,3))\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.heatmap(report.iloc[:-1, :-1].T, annot=True, cmap='coolwarm')\n",
        "    plt.title(f'{label} Report')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.subplot(1,3,2)\n",
        "    sns.heatmap(confusion_matrix(y_true=act, y_pred=pred), annot=True, cmap='coolwarm')\n",
        "    plt.title(f'{label} Confusion Matrix')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('Actual labels')\n",
        "\n",
        "    # AUC_ROC Curve\n",
        "    if (not(isinstance(model, SVC))):\n",
        "      plt.subplot(1,3,3)\n",
        "      plt.plot([0,1],[0,1],'k--')\n",
        "      plt.plot(fpr,tpr,label=f'AUC = {np.round(np.trapz(tpr,fpr),3)}')\n",
        "      plt.legend(loc=4)\n",
        "      plt.title(f'{label} AUC_ROC Curve')\n",
        "      plt.xlabel('False Positive Rate')\n",
        "      plt.ylabel('True Positive Rate')\n",
        "      plt.tight_layout()\n",
        "\n",
        "  # Plotting Feature Importance\n",
        "  try:\n",
        "    plt.figure(figsize=(21,3))\n",
        "    plt.bar(range(len(indices)),importance[indices])\n",
        "    plt.xticks(range(len(indices)), [feature[i] for i in indices])\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "  except:\n",
        "    pass\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ciPSX8d6-dQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "#fit the model\n",
        "# Fitting the model\n",
        "lr.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(lr, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "\n",
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=5)\n",
        "# Fit the Algorithm\n",
        "grid.fit(X_smote, y_smote)\n",
        "# Select the best hyperparameters found by GridSearchCV\n",
        "best_params = grid.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)\n",
        "# Predict on the model\n",
        "# Initiate model with best parameters\n",
        "lr_model2 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)\n",
        "lr_model2.fit(X_smote,y_smote)\n",
        "analyse_model(lr_model2, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used GridSearchcv for tuning the hyperparametres.GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes there in improvement of 1% in recall score for not risky patients(class 0) i.e. previously it was 67% now it is 68%. Others are same as previous."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 Random Forest Classifier"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "rf_model = RandomForestClassifier(random_state=2)\n",
        "# Fit the Algorithm\n",
        "rf_model.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "aJfl2zFrESL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(rf_model, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'n_estimators' : [100,150],\n",
        "        'max_depth' : [4,6,8],\n",
        "        'min_samples_split' : [50,80],\n",
        "        'min_samples_leaf' : [46,60]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "rf = GridSearchCV(rf_model, param_grid = grid, scoring = 'recall', cv=5)\n",
        "# Fit the Algorithm\n",
        "rf.fit(X_smote, y_smote)\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(rf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously for class 0 precision ,recall,f1 score :88%,84%,86% and for class 1 precision,recall,f1 score:26% ,33%,29%<BR>\n",
        "Now for class 0 precision,recall,f1 score:90%,70%,79% and for class 1 precision,recall,f1 score:25%,57%,35%.<br>Improvement in recall score for class 1 from 33% to 57% which is an important imporvement in this scenerio."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to completely avoid any situations where the patient has heart disease, a high recall is desired. Whereas if we want to avoid treating a patient with no heart diseases a high precision is desired.\n",
        "Assuming that in our case the patients who were incorrectly classified as suffering from heart disease are equally important since they could be indicative of some other ailment, so we want a balance between precision and recall and a high f1 score is desired.\n",
        "Since we have added synthetic datapoints to handle the huge class imbalance in training set, the data distribution in train and test are different so the high performance of models in the train set is due to the train-test data distribution mismatch and not due to overfitting.\n"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3  XGBClassifier"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from xgboost import XGBClassifier\n",
        "xgb_clf = XGBClassifier(random_state=2)\n",
        "# Fit the Algorithm\n",
        "xgb_clf.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(xgb_clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'n_estimators' : [50,80,100],\n",
        "        'max_depth' : [4,6,8],\n",
        "        'eta' : [0.05,0.08,0.1]\n",
        "        }\n",
        "\n",
        "# Fit the Algorithm\n",
        "# GridSearch to find the best parameters\n",
        "xgb = GridSearchCV(xgb_clf, param_grid = grid, scoring = roc_auc_score, cv=5,verbose=2)\n",
        "xgb.fit(X_smote, y_smote)\n",
        "# Predict on the model\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(xgb, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously for class 0 precision ,recall,f1 score :87%,92%,89% and for class 1 precision,recall,f1 score:33% ,23%,27%<BR>\n",
        "Now for class 0 precision,recall,f1 score:90%,76%,83% and for class 1 precision,recall,f1 score:28%,52%,36%.<br>Improvement in recall score for class 1 from 23% to 52% which is an important imporvement in this scenerio."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 4  Support Vector Classifier"
      ],
      "metadata": {
        "id": "koa0gZPhkl-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "svc= SVC(random_state= 0,probability=True)\n",
        "# Fit the Algorithm\n",
        "svc.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "s8yyYJ6hkfQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GRAjRCwAlsaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_model(svc, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "DBT0n59il1uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "KAbxGvermicX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'kernel': [\"rbf\",\"poly\",\"sigmoid\"],\n",
        "        'C': [0.1,10, 100],\n",
        "        'max_iter' : [1000]}\n",
        "svc_clf=SVC(probability=True)\n",
        "# GridSearch to find the best parameters\n",
        "svc_grid1=svc_grid = GridSearchCV(svc_clf, param_grid = grid, scoring = f1_score, cv=5,verbose=2)\n",
        "svc_grid1.fit(X_smote, y_smote)\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(svc_grid1, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "O7evj5OymHZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svc.best_params_"
      ],
      "metadata": {
        "id": "SKQLu-bnZKv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "Bp-d-nvumfGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "GridSearchCV which uses the Grid Search technique for finding the optimal hyperparameters to increase the model performance.\n",
        "\n",
        "our goal should be to find the best hyperparameters values to get the perfect prediction results from our model.  But the question arises, how to find these best sets of hyperparameters? One can try the Manual Search method, by using the hit and trial process and can find the best hyperparameters which would take huge time to build a single model.\n",
        "\n",
        "For this reason, methods like Random Search, GridSearch were introduced.\n",
        "Grid Search uses a different combination of all the specified hyperparameters and their values and calculates the performance for each combination and selects the best value for the hyperparameters. This makes the processing time-consuming and expensive based on the number of hyperparameters involved.\n",
        "\n",
        "In GridSearchCV, along with Grid Search, cross-validation is also performed. Cross-Validation is used while training the model.\n",
        "\n",
        "That's why I have used GridsearCV method for hyperparameter optimization."
      ],
      "metadata": {
        "id": "96v8kn26mfGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "4EPyvC3ImfGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously for class 0 precision ,recall,f1 score :91%,66%,76% and for class 1 precision,recall,f1 score:24% ,62%,35%<BR>\n",
        "Now for class 0 precision,recall,f1 score:91%,28%,43% and for class 1 precision,recall,f1 score:17%,83%,28%.<br>Improvement in recall score for class 1 from 62% to 83% which is an important imporvement in this scenerio."
      ],
      "metadata": {
        "id": "dcjc3pqCmfGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 5 KNN Classifer"
      ],
      "metadata": {
        "id": "NPoBTPiIBfDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_clf = KNeighborsClassifier()\n",
        "knn_clf.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "S_jSyYGVBg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "MuF9i9IZoQL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_model(knn_clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "_YC2-zuKoRcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "JJnV-cD1C0C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HYperparameter Grid\n",
        "grid = {'n_neighbors' : [5,7,9],\n",
        "        'metric' : ['minkowski','euclidean','manhattan']}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "knn = GridSearchCV(knn_clf, param_grid = grid, scoring = f1_score, cv=5,verbose=1)\n",
        "knn.fit(X_smote, y_smote)\n",
        "\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(knn, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "QwtwwPosCmL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "tUTfB7bapjvQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As such there is no improvement in the score after tuning hyperparameters."
      ],
      "metadata": {
        "id": "aOymbAjbpsdk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 6  Naive Bayes Classifier"
      ],
      "metadata": {
        "id": "cF1G5xPtqBY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nbc = GaussianNB()\n",
        "nbc.fit(X_smote,y_smote)"
      ],
      "metadata": {
        "id": "8w7odD9bDB53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ibhnJzQCqiQY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_model(nbc, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "V7kAB8jgpHn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 7  Decision Tree Classifier"
      ],
      "metadata": {
        "id": "Zdx3RPuHemMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(X_smote, y_smote)"
      ],
      "metadata": {
        "id": "6H_bmQoSsXoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "d5wYsSjce2CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyse_model(clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "CanmCujFswD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "x4S78seSeVqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HYperparameter Grid\n",
        "grid = {'criterion' : ['gini','entropy'],\n",
        "        'max_depth':[2,4,6,8],\n",
        "        'max_leaf_nodes':[2,4,6,8],\n",
        "\n",
        "       }\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "clf = GridSearchCV(clf, param_grid = grid, scoring = f1_score, cv=5,verbose=1)\n",
        "clf.fit(X_smote, y_smote)\n",
        "\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(clf, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "xg1e7uW-vDzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.best_params_"
      ],
      "metadata": {
        "id": "3-TOmo5Sg2w2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "eXA5oJANnRTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is great imrovement w.r.t. to precision score (class 1) is increased from 27% to 77%."
      ],
      "metadata": {
        "id": "s-7AsDLunThx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming that in our case the patients who were incorrectly classified as suffering from heart disease are equally important since they could be indicative of some other ailment, so we want a balance between precision and recall and a high f1 score is desired.<Br>If we want to completely avoid any situations where the patient has heart disease, a high recall is desired. Whereas if we want to avoid treating a patient with no heart diseases a high precision is desired.<BR>But a person having disease remain untreated is the dangerous situation so I have selected \"RECALL\" as important measure."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best performance of Models on test data based on evaluation metrics for class 1:<BR>\n",
        "Recall - SVC(83%),Decision tree(77%),Logistic Regression(71%)<BR>\n",
        "Precision - Naive Bayes Classifier<BR>\n",
        "F1 Score - Logistic Regression<BR>\n",
        "Accuracy - Loistic Regression"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model explainability refers to the concept of being able to understand the machine learning model. For example – If a healthcare model is predicting whether a patient is suffering from a particular disease or not. The medical practitioners need to know what parameters the model is taking into account or if the model contains any bias. So, it is necessary that once the model is deployed in the real world. Then, the model developers can explain the model.\n",
        "\n",
        "Popular techniques for model explainability:\n",
        "\n",
        "LIME\n",
        "SHAP\n",
        "ELI-5\n",
        "In this project I'll be using SHAP for model explainability. Among the various methods in SHAP I'll be using the SHAP summary plot, which plots features/columns in order of their impact on the predictions and also plots the SHAP values."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install shap==0.40.0\n",
        "import shap\n",
        "import graphviz\n",
        "sns.set_style('darkgrid')"
      ],
      "metadata": {
        "id": "be50npVOWOkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_summary = shap.kmeans(X, 100)\n",
        "# Create an explainer object\n",
        "explainer = shap.KernelExplainer(svc.predict_proba, X_summary)\n",
        "# Compute the SHAP values for all the samples in the test data\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "ZanwMBIzqXOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summery plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=X_cols.columns)\n",
        ""
      ],
      "metadata": {
        "id": "IFV1eZjKrAZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get shap values\n",
        "explainer = shap.TreeExplainer(svc)\n",
        "shap_values = explainer(X_test)"
      ],
      "metadata": {
        "id": "7RWGJO6tV_R4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Waterfall plot for first observation\n",
        "shap.plots.waterfall(shap_values[0])"
      ],
      "metadata": {
        "id": "FPXqZRdRXKhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that different importance or impact power is given to the features by each model and that defines how well the model performs on prediction. Random forest gives almost all the features a significant impact power and therefore it performs the best out of all the 4 models.\n",
        "\n",
        "By looking at the SHAP summary plot for each model, we can figure out the feature importance and also its impact power by understanding the SHAP values."
      ],
      "metadata": {
        "id": "OIjpTzSZuiU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "lUTqClQSuiU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "7wuGOrhz0itI",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "qjKvONjwE8ra",
        "ArJBuiUVfxKd",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "JWYfwnehpsJ1",
        "bmKjuQ-FpsJ3",
        "7AN1z2sKpx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/121deepti/Cardiovascular_Risk_Prediction/blob/main/Cardiovascular_Risk_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to predict the 10-year risk of future coronary heart disease (CHD) for patients in Framingham, Massachusetts. A dataset containing demographic, behavioral, and medical risk factors for over 4000 patients is used to build a predictive model. The model will use machine learning techniques to analyze the provided information and make accurate CHD risk predictions. The goal of the project is to develop a tool for early detection and prevention of CHD, addressing a significant public health concern. The outcome of the project will be a predictive model that can be used by healthcare providers to make informed decisions regarding patient care.\n",
        "\n",
        "There were approximately 3390 records and 17 attributes in the dataset.\n",
        "We started by importing the dataset, and necessary libraries and conducted exploratory data analysis (EDA).\n",
        "Outliers and null values were removed from the raw data and treated. Data were transformed to ensure that it was compatible with machine learning models.\n",
        "We handled target class imbalance using SMOTE.\n",
        "Then finally cleaned and scaled data was sent to 8 various models, the metrics were made to evaluate the model, and we tuned the hyperparameters to make sure the right parameters were being passed to the model.\n",
        "When developing a machine learning model, it is generally recommended to track multiple metrics because each one highlights distinct aspects of model performance. We are, focusing more on the Recall score and F1 score.\n",
        "It is categorically unacceptable to miss identifying a particular patient or to classify a particular patient as healthy (false negative). That is why we have preferred recall score."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/121deepti/Cardiovascular_Risk_Prediction/blob/main/Cardiovascular_Risk_Prediction.ipynb"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What exactly are cardiovascular diseases?**\n",
        "\n",
        "A group of conditions affecting the heart and blood vessels is known as cardiovascular diseases. They consist of heart disease, which affects the blood vessels that supply the heart muscle. The majority of the time, a blockage that prevents blood from flowing to the heart or brain is to blame for heart attacks and strokes, which are typically sudden events. A buildup of fatty deposits on the inner walls of the blood vessels that supply the heart or brain is the most common cause of this.\n",
        "\n",
        "The goal of the classification is to predict the 10-year risk of future coronary heart disease (CHD) for patients. The issue of coronary heart disease is a significant public health concern and early prediction of CHD risk is crucial for preventative measures. The dataset is from an ongoing cardiovascular study on residents of Flamingham, Massachusetts. The data set includes over 4000 records and 16 attributes, each of which is a potential risk factor, including demographic, behavioral, and medical risk factors.\n",
        "\n",
        "**WHY DO WE NEED CARDIOVASCULAR RISK PREDICTION?**\n",
        "\n",
        "The greatest obstacle facing the medical industry is accurately predicting and diagnosing heart disease. Heart diseases are influenced by numerous factors.\n",
        "Heart disease is even referred to as a \"silent killer\" because it kills people without showing any obvious symptoms.\n",
        "When high-risk patients are diagnosed with heart disease early, it is easier to make lifestyle changes, which in turn lowers the risk of complications.\n",
        "Based on the way people currently live, machine learning can help predict the likelihood of heart disease in the coming years."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import  make_scorer,f1_score,roc_curve,accuracy_score,classification_report,confusion_matrix,roc_auc_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBRFClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from imblearn.combine import SMOTETomek"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AFSKXkw26Fy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df=pd.read_csv('/content/drive/MyDrive/my_data/data_cardiovascular_risk.csv')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.sample(3)"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Missing values percentage\n",
        "round((df.isna().sum().sort_values(ascending=False))*100/df.shape[0],2)"
      ],
      "metadata": {
        "id": "ZLvDxIZ29DJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(),cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-profiling"
      ],
      "metadata": {
        "id": "4EPfVxWR_0Kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas_profiling import ProfileReport\n",
        "profile = ProfileReport(df, title=\"Pandas Profiling Report\")\n",
        "profile"
      ],
      "metadata": {
        "id": "3fAyRlYdwAeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 3390 rows and 17 columns in the dataset. No duplicates are found in the dataset.Some Null values are observed in the features."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset provides the patients’ information. It includes over 3,390 records and 17 attributes(from which TenYearCD is the target column). Variables Each attribute is a potential risk factor. There are demographic, behavioural, and medical risk factors\n",
        "\n",
        "Demographic:\n",
        "*   Sex: male or female (\"M\" or \"F\")\n",
        "*   Age: Age of the patient (Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n",
        "*   Education: The level of education of the patient (categorical values - 1,2,\n",
        "3,4)\n",
        "\n",
        "Behavioral:<br>\n",
        "*   is_smoking: whether or not the patient is a current smoker (\"YES\" or \"NO\")\n",
        "*   Cigs Per Day: the number of cigarettes that the person smoked on average in one day\n",
        "\n",
        "Medical(History):\n",
        "\n",
        "\n",
        "*   BP Meds: whether or not the patient was on blood pressure medication\n",
        "*   Prevalent Stroke: whether or not the patient had previously had a stroke\n",
        "*   Prevalent Hyp: whether or not the patient was hypertensive\n",
        "*   Diabetes: whether or not the patient had diabetes (Nominal) Medical(current)\n",
        "*   Tot Chol: total cholesterol level\n",
        "*   Sys BP: systolic blood pressure\n",
        "*   Dia BP: diastolic blood pressure\n",
        "*   BMI: Body Mass Index\n",
        "*   Heart Rate: heart rate\n",
        "*   Glucose: glucose level\n",
        "*   TenYearCHD(**Target Variable**): 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in df.columns:\n",
        "  print(\"No of Unique Values in \", i, \" is:\", df[i].nunique())"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a copy of dataset\n",
        "df_eda=df.copy()"
      ],
      "metadata": {
        "id": "wuJMctAK-w9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevalent stroke effect on heart disease\n",
        "pd.crosstab(df_eda.prevalentStroke,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "7ly3THlQ2GFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevalent Hypertension impact on heart disease\n",
        "pd.crosstab(df_eda.prevalentHyp,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "7noclZVw3HKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating age bins and impact on heart disease\n",
        "df_eda['age_bins'] = pd.cut(x=df_eda['age'], bins=[30, 35, 40, 45,50,55,60,65,70])\n",
        "# We can check the frequency of each bin\n",
        "pd.crosstab(df_eda.age_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "6c3_E6kL56Of"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating BMI bins and finding its relation with heart disease\n",
        "df_eda['BMI_bins'] = pd.cut(x=df_eda['BMI'], bins=[15,25,35,45,55,65])\n",
        "print(pd.crosstab(df_eda.BMI_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.BMI_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "TzbD843vG2_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating cigrette bins and finding its relation with heart disease\n",
        "df_eda['cig_bins'] = pd.cut(x=df_eda['cigsPerDay'], bins=[10, 20,30,40,50,60])\n",
        "\n",
        "print(pd.crosstab(df_eda.cig_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.cig_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "wL6n1i-KI-nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Prevalent Hypertension impact on heart disease\n",
        "pd.crosstab(df_eda.sex,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "3vGrwHxBFfGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Diebetes impact on heart disease\n",
        "pd.crosstab(df_eda.diabetes,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)\n"
      ],
      "metadata": {
        "id": "RJ6x-JORAHwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BP medication impact on heart disease\n",
        "pd.crosstab(df_eda.BPMeds,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "H4rYPoADE2ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating cholestrol bins and finding its relation with heart disease\n",
        "df_eda['chol_bins'] = pd.cut(x=df_eda['totChol'], bins=[100, 200,300,400,500,600,700])\n",
        "print(pd.crosstab(df_eda.chol_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.chol_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "ndujkExlL4qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating heartRate bins and finding its relation with heart disease\n",
        "df_eda['heartRate_bins'] = pd.cut(x=df_eda['heartRate'], bins=[40,60,80,100,120,140,160])\n",
        "print(pd.crosstab(df_eda.heartRate_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.heartRate_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "t_MxWGENPMnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating Glucose bins and finding its relation with heart disease\n",
        "df_eda['Glucose_bins'] = pd.cut(x=df_eda['glucose'], bins=[40,150,250,350,450])\n",
        "# We can check the frequency of each bin\n",
        "print(pd.crosstab(df_eda.Glucose_bins,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.Glucose_bins,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "rTVndYhoQBgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.glucose.min()"
      ],
      "metadata": {
        "id": "ZvxrZ4p-O7Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(2)"
      ],
      "metadata": {
        "id": "w6cvImdAK3RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1=df[df['TenYearCHD']==0]\n",
        "df1[['age','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']].describe()"
      ],
      "metadata": {
        "id": "T9LX4OfHMD5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2=df[df['TenYearCHD']==1]\n",
        "df2[['age','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']].describe()"
      ],
      "metadata": {
        "id": "JVSJ03EnGscV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df[df['TenYearCHD']==1]))\n",
        "print(len(df[df['TenYearCHD']==0]))"
      ],
      "metadata": {
        "id": "Fmzb_XZYROqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pd.crosstab(df_eda.is_smoking,df_eda.TenYearCHD))\n",
        "print('\\n')\n",
        "pd.crosstab(df_eda.is_smoking,df_eda.TenYearCHD).apply(lambda r: round((r/r.sum()),2), axis=1)"
      ],
      "metadata": {
        "id": "nvfqC26bKq-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Univariate Analysis"
      ],
      "metadata": {
        "id": "X1kE-j4oR7SM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols=['age','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']\n",
        "cate_cols=['sex','education','is_smoking','BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes','TenYearCHD']"
      ],
      "metadata": {
        "id": "HHK7kDywSLNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 Count plot of Categorical columns\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(cate_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.countplot(x =item, data = df)\n",
        "  for p in ax.patches:\n",
        "    ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
        "  plt.tight_layout()\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps in analyzing the categorical features count in the data set."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The findings are:\n",
        "*   There are more number of females as compared to male.\n",
        "*   Almost equal ratio between smoker and non-smoker.\n",
        "*   There are more number of non-BP patients as compared to BP patients.\n",
        "*   The persons who history of prevalent stroke are very less.\n",
        "*   Hypertension and Diabetic patients are less in number.\n",
        "*   There are more number of patients who are in less risk zone."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 Pie charts for Categorical Columns\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(cate_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  df[item].value_counts().plot(kind='pie',autopct='%1.0f%%')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the percentage of distribution of categorical data"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ratio is as follows:\n",
        "*   57% female and 43% males\n",
        "*   50% smokers and 50% non-smokers\n",
        "*   97% NonBP and NonDiebitic patients while 3% BP and Diebitic patients\n",
        "*   99% have Stroke history\n",
        "*   68% non-Hypertension Patients  \n",
        "*   85% are under safe category"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 Histogram for Numerical columns\n",
        "fig = plt.figure(figsize=(12, 10))\n",
        "for index,item in enumerate(numeric_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.distplot(df[item])\n",
        "  sk=round(df[item].skew(),2)\n",
        "  ax=fig.gca()\n",
        "  ax.axvline(df[item].mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(df[item].median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(item+'  skewness'+str(sk))\n",
        "  plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart displays the PDF of numerical variables with their skewness."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are some important findings-\n",
        "*   Except cigs_per_day all other are following Normal or almost Normal Distribution\n",
        "*   All features are Positively skewed but cigs_per_day and BMI highly skewed but Glusose is very high skewed ~6.14."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 Log Transformation of skewed variable\n",
        "fig = plt.figure(figsize=(8, 3))\n",
        "plt.subplot(1,3,1)\n",
        "ax=fig.gca()\n",
        "sns.distplot(np.log1p(df['BMI']))\n",
        "sk=round((np.log1p(df['BMI'].skew())),2)\n",
        "ax.set_title(\"BMI skewness \"+str(sk))\n",
        "\n",
        "plt.subplot(1,3,2)\n",
        "ax=fig.gca()\n",
        "sns.distplot(np.log1p(df['cigsPerDay']))\n",
        "sk=round((np.log1p(df['cigsPerDay'].skew())),2)\n",
        "ax.set_title(\"cigsPerDay \"+str(sk))\n",
        "\n",
        "plt.subplot(1,3,3)\n",
        "ax=fig.gca()\n",
        "sns.distplot(np.log1p(df['glucose']))\n",
        "sk=round((np.log1p(df['glucose'].skew())),2)\n",
        "ax.set_title(\"Glucose \"+str(sk))"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart helps to show the variable distribution after log transformation."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying log transformation, BMI skewness is reduced to ~ 0.7,Cigs perday to ~0.8 and Glucose to ~1.97."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 Box plot Analysis\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "for index,item in enumerate(numeric_cols):\n",
        "  plt.subplot(3,3,index+1)\n",
        "  ax = fig.gca()\n",
        "  plt.xlabel(item)\n",
        "  sns.boxplot(x =item, data = df)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart analyses the outliers present in numerical features."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here the outcomes of this chart-\n",
        "*   \"Age\"-No outlier is present\n",
        "*   \"cigs Per Day\" -Two outliers are present beyond the upper limit.\n",
        "*   \"totChol\", \"diaBP\",\"heartRate\" and \"glucose\"-Many outliers are present beyond the upper limit and some are present on the lower boundary too.\n",
        "*   \"SysBP\" and \"BMI\"-Many outliers are present beyond the upper boundary."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Outliers can mislead the analysis of data that can give wrong signals to the business."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Bivariate Analysis"
      ],
      "metadata": {
        "id": "qnk1IpK_B2c9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 Relation of SystolicBP and diastolicBP\n",
        "print (numeric_cols)\n",
        "print(cate_cols)\n",
        "fig = plt.figure(figsize=(6, 4))\n",
        "sns.scatterplot(data=df,x='sysBP',y='diaBP')"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart shows the relationship between sysBP and diaBP."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is linear relationship between sysBP and diaBP."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "sysBP and diaBP are crucial factor in determining the risk of heart disease, so they should be in controllled manner."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 Bar graphs for categorical vs numerical features analysis\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "plt.subplot(3,4,1)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='BPMeds',y='sysBP',hue='prevalentStroke')\n",
        "\n",
        "plt.subplot(3,4,2)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='prevalentStroke',y='BMI',hue='sex')\n",
        "\n",
        "plt.subplot(3,4,3)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='prevalentHyp',y='totChol',hue='TenYearCHD')\n",
        "\n",
        "plt.subplot(3,4,4)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='diabetes',y='glucose',hue='TenYearCHD')\n",
        "\n",
        "plt.subplot(3,4,5)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='TenYearCHD',y='cigsPerDay')\n",
        "\n",
        "plt.subplot(3,4,6)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='TenYearCHD',y='totChol')\n",
        "\n",
        "plt.subplot(3,4,7)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='TenYearCHD',y='sysBP')\n",
        "\n",
        "plt.subplot(3,4,8)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='TenYearCHD',y='BMI')\n",
        "\n",
        "plt.subplot(3,4,9)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='TenYearCHD',y='glucose',hue='diabetes')\n",
        "\n",
        "plt.subplot(3,4,10)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='diabetes',y='heartRate')\n",
        "\n",
        "plt.subplot(3,4,11)\n",
        "ax = fig.gca()\n",
        "sns.barplot(data=df,x='prevalentStroke',y='heartRate')\n",
        "\n",
        "plt.subplot(3,4,12)\n",
        "ax = fig.gca()\n",
        "#sns.barplot(data=df,x='is_smoking',y='heartRate')\n",
        "#sns.barplot(data=df,x='TenYearCHD',y='age')\n",
        "\n",
        "plt.tight_layout()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart reflects the relationship between numerical and categorical variables."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are the insights-\n",
        "*   The persons on BP medicines and storke history having lower sysBP with those having no stroke history, while the persons not on BP medicine and stroke history having higher sysBP with those having no sysBP that means BP medicine plays an imporatnt role.\n",
        "*  There are higher BMI of females having stroke history but in case of no stroke the males have higher BMI .It signifies that increased BMI increses the risk of stroke.\n",
        "*   The persons having stroke history also have higher level of cholestrol that increases the risk of heart diseases.\n",
        "*   There is high risk of heart disease for diebitic persons.\n",
        "*   Smoking factor,Cholostrol,BP,BMI and Glucose level infulences high risk factor.\n",
        "*   Diebitic,Smokers having comparatively higher heart rate but prevalent stroke patients have significantly lower heart rate."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This analysis helps in recognizing the crucial factors that have strong infulence in determing heart disease."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.is_smoking.value_counts()"
      ],
      "metadata": {
        "id": "WMswjQFSVkKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "fig = plt.figure(figsize=(10, 12))\n",
        "plt.subplot(5,4,1)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,x='cigsPerDay',y='is_smoking')\n",
        "\n",
        "plt.subplot(5,4,2)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,y='glucose',x='diabetes',hue='TenYearCHD')\n",
        "\n",
        "plt.subplot(5,4,3)\n",
        "ax = fig.gca()\n",
        "sns.boxplot(data=df,y='sysBP',x='BPMeds')\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+4)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='sysBP',x=item)\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes','TenYearCHD','BPMeds','sex']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+7)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='BMI',x=item)\n",
        "\n",
        "x=['prevalentStroke','prevalentHyp','diabetes','TenYearCHD']\n",
        "for i,item in enumerate(x):\n",
        "  plt.subplot(5,4,i+13)\n",
        "  ax = fig.gca()\n",
        "  sns.boxplot(data=df,y='heartRate',x=item)\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This helps in understanding the presence of outliers in various sceniors."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the insights as per my undersatnding\n",
        "*   Some persons are smoking beyond the limit.\n",
        "*   In case of Non-Diebitic persons lot of outliers are present in their glucose level.\n",
        "*   Persons not on BP medication, no stroke history and non-diebitic having a lot high range of outliers for sysBP are present but high range of outliers are seen in prevalent as well as non prevalent hypertension patients.\n",
        "*   In BMI case, high range of outliers are present in high risk and low risk patients.BP medicated ,Males having lesser outliers comparatively.No stroke history patients,Non Diebitic having higher outliers.For HyperTension or not BMI showing high range of outliers present.\n",
        "*   In HeartRate case,Non prevalent stroke,Non diabetic having higher range of outliers,For HyperTension or not Heart Rate showing high range of outliers present and same in case of risk factor."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Persons not on BP medications,not Diebitic and no stroke history having outliers so these must be inspected carefully otherwise can lead to wrong prediction."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 Relation between Education and Sex with TenYearCHD\n",
        "pd.crosstab(df.education,df.TenYearCHD).plot(kind='pie',autopct='%1.0f%%',subplots=True)\n",
        "pd.crosstab(df.sex,df.TenYearCHD).plot(kind='pie',autopct='%1.0f%%',subplots=True)"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It shows the relation with heart risk with education."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As the education level increases the percentage of high risk patients decreases or remain equal.\n",
        "Majority of the patients belong to the education level 1, followed by 2, 3, and 4 respectively.\n",
        "Males have higher percentage of heart disease while females are in safer zone."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hypertensive heart disease is a long-term condition that develops over many years in people who have high blood pressure that increases the risk of cardiovascular diseases."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 correlation heatmap\n",
        "plt.figure(figsize=(12,12))\n",
        "correlation =df.corr()\n",
        "sns.heatmap(abs(correlation), annot=True)"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart is important to know the correlation between various dependent and independent features."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some important insights are-\n",
        "*   sysBP and DiaBP is highly correlated with prevalent Hypertension and BMI.\n",
        "*   Glucose is highly correlated with diabetes.\n",
        "*   sysBP and diaBP is highly correlated.\n",
        "*   age is correlated with prevalent hypertension,Choloestrol ,sysBP and diaBP as well.\n",
        "*   BP meds is correlated with sysBP,diaBP,hypertension\n",
        "*   TenYearCHD is correlated with age,sysBP,diaBP,glucose and hypertension."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These correlation must to know as it saves us from the pitfall of bias-variance and multicollinearity."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating copy of data frame\n",
        "df_cpy=df.copy()"
      ],
      "metadata": {
        "id": "f8S_uvzEgm8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# features which has less than 5%  null values present.\n",
        "nan_columns = ['education', 'cigsPerDay', 'BPMeds', 'totChol', 'BMI', 'heartRate']\n",
        "\n",
        "# dropping null values\n",
        "df_cpy.dropna(subset=nan_columns, inplace=True)\n",
        "\n",
        "#glucose has ~8% null values\n",
        "df_cpy['glucose'] = df_cpy.glucose.fillna(df_cpy.glucose.median())\n",
        "\n",
        "df_cpy.isna().sum().sum()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Typically, we use other records to replace these null values. However, the entries in this dataset are person-specific. The values vary from person to person, and the dataset is related to the medical field in this particular instance. Consequently, removing rows with any null value is the most logical choice we have for dealing with such values.\n",
        "\n",
        "We cannot take any risks with this prediction, so if we attempt to impute null values using advanced methods, it may affect the outcome because the values will be incorrect.\n",
        "\n",
        "In the healthcare industry, every piece of data is crucial. Because of this, we came up with a solution by setting a threshold value. If a feature has less than 5% null values, we decide to drop those rows, and the remaining rows are imputing, which will affect prediction but not significantly.As in case of glucose ~8% null values and outliers are present so we replace it with median value.\n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_cpy.describe().T"
      ],
      "metadata": {
        "id": "0JpkS1iLtsGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen in the statistical summary for numerical features, there is a significant difference between the 75% percentile and maximum value, indicating that the dataset contains skewness and outliers."
      ],
      "metadata": {
        "id": "jEgdjP4llIWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# figsize\n",
        "plt.figure(figsize=(10,5))\n",
        "# boxplot of numerical features\n",
        "sns.boxplot(data=df_cpy[numeric_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EayF3mbllHfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As lot of outliers are present since we have limited datapoint hence we are not simply removing the outlier instead of that we are using the clipping method."
      ],
      "metadata": {
        "id": "lkBUmSoil_LV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "def clip_outliers(risk_df):\n",
        "    for col in risk_df[numeric_cols]:\n",
        "        # using IQR method to define range of upper and lower limit.\n",
        "        q1 = risk_df[col].quantile(0.25)\n",
        "        q3 = risk_df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_bound = q1 - 1.5 * iqr\n",
        "        upper_bound = q3 + 1.5 * iqr\n",
        "\n",
        "        # replacing the outliers with upper and lower bound\n",
        "        risk_df[col] = risk_df[col].clip(lower_bound, upper_bound)\n",
        "    return risk_df"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using the function to treat outliers\n",
        "df_cpy = clip_outliers(df_cpy)"
      ],
      "metadata": {
        "id": "NFhlr7odm_7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#BoxPlot after clipping outliers\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.boxplot(data=df_cpy[numeric_cols])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPrSr4Qknzwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented **clipping method**.  In this method, we set a cap on our outliers data, which means that if a value is higher than or lower than a certain threshold, all values will be considered outliers. This method replaces values that fall outside of a specified range with either the minimum or maximum value within that range.\n",
        "Here we have set the threshold of .25-1.5*IQR(Lower limit) and .75+1.5*IQR(Upper limit)."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "df_cpy['sex']=df_cpy['sex'].map({'M':1,'F':0})\n",
        "df_cpy['is_smoking']=df_cpy['is_smoking'].map({'YES':1,'NO':0})"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the datatypes of each column in the DataFrame\n",
        "df_cpy.dtypes"
      ],
      "metadata": {
        "id": "QDFMhid1lvld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one-hot encode the 'education' feature\n",
        "education_onehot = pd.get_dummies(df_cpy['education'], prefix='education',drop_first=True)\n",
        "\n",
        "# drop the original education feature\n",
        "df_cpy.drop('education', axis=1, inplace=True)\n",
        "\n",
        "# concatenate the one-hot encoded education feature with the rest of the data\n",
        "df_cpy = pd.concat([df_cpy, education_onehot], axis=1)\n",
        "df_cpy.head(3)"
      ],
      "metadata": {
        "id": "7RzHZpodu1AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "'BPMeds','prevalentStroke', 'prevalentHyp', 'diabetes'and 'TenYearCHD' are categorical type of features but already have numeric values.We have encoded the \"sex\" and \"is_smoking\" columns to number.As \"education\" column has 4 unique values ,we converted it to object data type and perform one hot encoding to it  "
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)<BR>\n",
        "**As in dataset we have no textual data so we have skipped this step.**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVkZv3D5y-Xt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "#create a new feature MAP(Mean Arterial Pressure) by using sysBP and diaBP\n",
        "df_cpy['Pulse_Pressure']=df_cpy['sysBP']-df_cpy['diaBP']\n",
        "#Dropping sysBP and DiaBP columns\n",
        "df_cpy.drop(['sysBP','diaBP'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking data, weather the provide information is correct or not\n",
        "df_cpy[(df_cpy.is_smoking == 1) & (df_cpy.cigsPerDay == 0)]\n",
        "# droping is_smoking column due to multi-collinearity\n",
        "df_cpy.drop('is_smoking', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "HuQBDZCpKdOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping ID column as not relevant\n",
        "df_cpy.drop('id',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "fWjdgutNpGT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating the numeric and categorical columns list\n",
        "numeric_cols.remove('sysBP')\n",
        "numeric_cols.remove('diaBP')\n",
        "cate_cols.remove('is_smoking')\n",
        "numeric_cols.append('Pulse_Pressure')"
      ],
      "metadata": {
        "id": "ppyl9z4yqKOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting correlation heatmap to check multicollinearity.\n",
        "plt.figure(figsize=(15,4))\n",
        "sns.heatmap(df_cpy.corr(),annot=True)"
      ],
      "metadata": {
        "id": "dxOK4Jz1tRnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating VIF\n",
        "def calc_vif(X):\n",
        "   vif = pd.DataFrame()\n",
        "   vif[\"variables\"] = X.columns\n",
        "   vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "   return(vif)"
      ],
      "metadata": {
        "id": "Cbi-wpSsNdXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking VIF of all the columns after excluding high VIF columns\n",
        "calc_vif(df_cpy[[i for i in df_cpy.describe().columns  if i not in['glucose','BMI','heartRate','totChol','Pulse_Pressure']]])"
      ],
      "metadata": {
        "id": "6gwIeKY5R7s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing high VIF columns and create a new data frame named df_removed\n",
        "df_removed=df_cpy.drop(['Pulse_Pressure','glucose','BMI','totChol','heartRate'],axis=1)"
      ],
      "metadata": {
        "id": "uDvFOBXISnIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating the numeric column list\n",
        "del numeric_cols[2:]\n",
        "numeric_cols"
      ],
      "metadata": {
        "id": "eNnLLf9ZzMv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed.shape"
      ],
      "metadata": {
        "id": "uw2BCT396i-R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all we have checked the VIF of all features and remove the features which are having high VIF(less than 10) and less important wrt. target variable.\n",
        "    This step is necessary as it saves our model from overfitting by removing multicollnerity."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After removing collinear features the data frame is left with 11 features which are important for building the model.From which \"age\" is the most important feature highly correlated with the target variable.  "
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in numeric_cols:\n",
        "  fig=plt.figure(figsize=(4,5))\n",
        "  ax=fig.gca()\n",
        "  feature= (df_removed[col])\n",
        "  sns.distplot(df_removed[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col+' '+str(feature.skew()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_removed['cigsPerDay']=np.log1p(df_removed['cigsPerDay'])\n",
        "fig=plt.figure(figsize=(4,5))\n",
        "ax=fig.gca()\n",
        "sns.distplot(df_cpy['cigsPerDay'])\n",
        "ax.axvline(df_cpy['cigsPerDay'].mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "ax.axvline(df_cpy['cigsPerDay'].median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "ax.set_title('cigsPerDay'+' '+str(df_removed['cigsPerDay'].skew()))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2KAMXK_dqLS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the numeric features after outliers removal are almost following Gaussian Distribution and having skewness less than 0.5 which seems quite normal.\n",
        "  But \"cigsPerDay\" having skewness >1 so i have applied log tranforamtion to make it follow Gaussian Distribution and finally its skewness is reduced less than 0.5"
      ],
      "metadata": {
        "id": "kpcRHbyKyPFK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Created X and y dataset\n",
        "#creating X(independent features) and y(target feature)\n",
        "X_cols=df_removed.copy()\n",
        "y=df_removed.TenYearCHD\n",
        "X_cols.drop('TenYearCHD',axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "mmhK5FRxuQgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scalar=StandardScaler()\n",
        "X=scalar.fit_transform(X_cols)\n",
        "y=y"
      ],
      "metadata": {
        "id": "H17v1ydL0e17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Standard Scaler to scale the indendepent features as all the numeric features are following Gaussian Distribution."
      ],
      "metadata": {
        "id": "1ryQ9N0CJ5Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per my knowledge, for this dataset dimensionality reduction is not required.\n",
        "\n",
        "Essentially where high dimensions are a problem or where it is a particular point in the algorithm to dimension reduction.\n",
        "\n",
        "Hard rules are hard to state, other than “after you have tried it, did it improve matters”, which isn’t always the most useful guidance.\n",
        "\n",
        "Instead, looking at why we might want to do this we can get a bit of insight. Admittedly some of the following might blur together a bit at the edges but the aim is to give a flavour.\n",
        "\n",
        "1. Our data are too big. 4 million rows. 50,000 columns… is there a lot of redundancy there? Building a model on this could be very expensive. Even relatively simple dimension reduction techniques like PCA can capture almost all of the information in a fraction of the memory if there are strong relationships (that can be linearly approximated) in the data.\n",
        "\n",
        "2. We are over-fitting. If you build a model with tens of thousands of degrees of freedom but don’t have a lot of examples you can easily overfit. Dimension reduction is one way of handling this, though often not the the best\n",
        "\n",
        "3. We want to bring in external data. OK, this is a bit different but worth a note. In applications like word2vec we want to build a classifier using an embedding. We may want to classify some text into different categories but with only a limited number of examples. The complexity of free text is vast but a low dimension embedding is much smaller and will not overfit so badly in a classifier. Building a low dimensional embedding on external text, applying it to the text to be classified then building a classifier is using dimension reduction to bring in external data.\n",
        "\n",
        "4. We suffer from the curse of dimesnionality. Consider something like a nearest neighbour search. As the number of dimensions gets large we see some unwanted behaviour, especially if we are looking at things like euclidean distances. Projecting your data to a lower dimensional space for nearest neighbour, clustering or outlier detection can be both more robust and more meaningful.\n",
        "\n",
        "5. Some tools are all about this. Collaborative filtering through matrix factorisation is an example. Can we approximately describe behaviour as a linear combination of a smaller number of preferences/behaviours?"
      ],
      "metadata": {
        "id": "greCxziR1TgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# split into 70:30 ratio\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 0,stratify=y)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are two competing concerns: with less training data, your parameter estimates have greater variance. With less testing data, your performance statistic will have greater variance. Broadly speaking you should be concerned with dividing data such that neither variance is too high, which is more to do with the absolute number of instances in each category rather than the percentage.\n",
        "\n",
        "If you have a total of 100 instances, you're probably stuck with cross validation as no single split is going to give you satisfactory variance in your estimates. If you have 100,000 instances, it doesn't really matter whether you choose an 80:20 split or a 90:10 split (indeed you may choose to use less training data if your method is particularly computationally intensive).\n",
        "\n",
        "You'd be surprised to find out that 80/20 is quite a commonly occurring ratio, often referred to as the Pareto principle. It's usually a safe bet if you use that ratio.\n",
        "\n",
        "In this case the training dataset is small, that's why I have taken 70:30 ratio.\n",
        "As my target variable is highly imbalanced i have used Stratified Sampling so that training and testing set get equal proportion of 1 and 0 class."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Dependant Column Value Counts\n",
        "print(df_removed.TenYearCHD.value_counts())\n",
        "print(\" \")\n",
        "# Dependant Variable Column Visualization\n",
        "df_removed['TenYearCHD'].value_counts().plot(kind='pie',\n",
        "                              figsize=(15,6),\n",
        "                               autopct=\"%1.1f%%\",\n",
        "                               startangle=90,\n",
        "                               shadow=True,\n",
        "                               labels=['Not at Risk(%)','at Risk(%)'],\n",
        "                               colors=['skyblue','red'],\n",
        "                               explode=[0,0]\n",
        "                              )"
      ],
      "metadata": {
        "id": "AFHLSMeA2bKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here one can easily notice that ~85% persons are safe only ~15% are at risk, so the ratio is 85:15 which is the sign of unbalanced dataset."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "# Handling class imbalance by oversampling followed by removing the Tomek link\n",
        "X_smote, y_smote = SMOTETomek(random_state=42).fit_resample(X_train, y_train)\n",
        "# Checking Value counts for both classes Before and After handling Class Imbalance:\n",
        "for col,label in [[y_train,\"Before\"],[y_smote,'After']]:\n",
        "  print(label+' Handling Class Imbalace:')\n",
        "  print(col.value_counts(),'\\n')"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used SMOTE (Synthetic Minority Over-sampling technique) followed by removing the Tomek link for balanced the 85:15 dataset.\n",
        "\n",
        "SMOTE is a technique in machine learning for dealing with issues that arise when working with an unbalanced data set. In practice, unbalanced data sets are common and most ML algorithms are highly prone to unbalanced data so we need to improve their performance by using techniques like SMOTE.\n",
        "\n",
        "To address this disparity, balancing schemes that augment the data to make it more balanced before training the classifier were proposed. Oversampling the minority class by duplicating minority samples or undersampling the majority class is the simplest balancing method.\n",
        "\n",
        "The idea of incorporating synthetic minority samples into tabular data was first proposed in SMOTE, where synthetic minority samples are generated by interpolating pairs of original minority points.\n",
        "\n",
        "SMOTE is a data augmentation algorithm that creates synthetic data points from raw data. SMOTE can be thought of as a more sophisticated version of oversampling or a specific data augmentation algorithm.\n",
        "\n",
        "SMOTE has the advantage of not creating duplicate data points, but rather synthetic data points that differ slightly from the original data points. SMOTE is a superior oversampling option.\n",
        "\n",
        "That's why for lots of advantages, I have used SMOTE technique for balancing the dataset."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to train the input model and print evaluation matrix\n",
        "def analyse_model(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "  '''Takes classifier model and train test splits as input and prints the\n",
        "  evaluation matrices with the plot and returns the model'''\n",
        "\n",
        "  # Fitting the model\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  # Feature importances\n",
        "  try:\n",
        "    try:\n",
        "      importance = model.feature_importances_\n",
        "      feature = features\n",
        "    except:\n",
        "      importance = np.abs(model.coef_[0])\n",
        "      feature = features\n",
        "    indices = np.argsort(importance)\n",
        "    indices = indices[::-1]\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "  # Plotting Evaluation Metrics for train and test dataset\n",
        "  for x, act, label in ((X_train, y_train, 'Train-Set'),(X_test, y_test, \"Test-Set\")):\n",
        "\n",
        "    # Getting required metrics\n",
        "    pred = model.predict(x)\n",
        "    pred_proba = model.predict_proba(x)[:,1]\n",
        "    report = pd.DataFrame(classification_report(y_true=act,y_pred=pred, output_dict=True))\n",
        "    fpr, tpr, thresholds = roc_curve(act, pred_proba)\n",
        "\n",
        "    # Classification report\n",
        "    plt.figure(figsize=(18,3))\n",
        "    plt.subplot(1,3,1)\n",
        "    sns.heatmap(report.iloc[:-1, :-1].T, annot=True, cmap='coolwarm')\n",
        "    plt.title(f'{label} Report')\n",
        "\n",
        "    # Confusion Matrix\n",
        "    plt.subplot(1,3,2)\n",
        "    sns.heatmap(confusion_matrix(y_true=act, y_pred=pred), annot=True, cmap='coolwarm')\n",
        "    plt.title(f'{label} Confusion Matrix')\n",
        "    plt.xlabel('Predicted labels')\n",
        "    plt.ylabel('Actual labels')\n",
        "\n",
        "    # AUC_ROC Curve\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.plot(fpr,tpr,label=f'AUC = {np.round(np.trapz(tpr,fpr),3)}')\n",
        "    plt.legend(loc=4)\n",
        "    plt.title(f'{label} AUC_ROC Curve')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.tight_layout()\n",
        "\n",
        "  # Plotting Feature Importance\n",
        "  try:\n",
        "    plt.figure(figsize=(21,3))\n",
        "    plt.bar(range(len(indices)),importance[indices])\n",
        "    plt.xticks(range(len(indices)), [feature[i] for i in indices])\n",
        "    plt.title('Feature Importance')\n",
        "    plt.tight_layout()\n",
        "  except:\n",
        "    pass\n",
        "  plt.show()\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "ciPSX8d6-dQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "analyse_model(lr, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# Define the hyperparameter grid\n",
        "\n",
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=5)\n",
        "# Fit the Algorithm\n",
        "grid.fit(X_smote, y_smote)\n",
        "# Select the best hyperparameters found by GridSearchCV\n",
        "best_params = grid.best_params_\n",
        "print(\"Best hyperparameters: \", best_params)\n",
        "# Predict on the model\n",
        "# Initiate model with best parameters\n",
        "lr_model2 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)\n",
        "analyse_model(lr_model2, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "rf_model = RandomForestClassifier(random_state=2)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "analyse_model(rf_model, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "aJfl2zFrESL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'n_estimators' : [100,150],\n",
        "        'max_depth' : [4,6,8],\n",
        "        'min_samples_split' : [50,80],\n",
        "        'min_samples_leaf' : [46,60]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "rf = GridSearchCV(rf_model, param_grid = grid, scoring = scoring, cv=5)\n",
        "# Fit the Algorithm\n",
        "rf.fit(X_smote, y_smote)\n",
        "\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(rf.best_estimator_, X_smote, X_test, y_smote, y_test)\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "from xgboost import XGBClassifier\n",
        "clf = XGBClassifier(random_state=2)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "analyse_model(clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Calculating accuracy on train and test\n",
        "train_accuracy = accuracy_score(y_smote,train_class_preds)\n",
        "test_accuracy = accuracy_score(y_test,test_class_preds)\n",
        "\n",
        "print(classification_report(train_class_preds, y_smote))\n",
        "print(\" \")\n",
        "print(\"roc_auc_score\")\n",
        "print(roc_auc_score(y_smote, train_class_preds))\n",
        "\n",
        "print(classification_report(test_class_preds, y_test))\n",
        "print(\" \")\n",
        "print(\"roc_auc_score\")\n",
        "print(roc_auc_score(y_test, test_class_preds))"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'n_estimators' : [50,80,100],\n",
        "        'max_depth' : [4,6,8],\n",
        "        'eta' : [0.05,0.08,0.1]\n",
        "        }\n",
        "\n",
        "# Fit the Algorithm\n",
        "# GridSearch to find the best parameters\n",
        "xgb = GridSearchCV(clf, param_grid = grid, scoring = roc_auc_score, cv=5,verbose=2)\n",
        "xgb.fit(X_smote, y_smote)\n",
        "# Predict on the model\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(xgb.best_estimator_, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 4"
      ],
      "metadata": {
        "id": "koa0gZPhkl-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 4 Implementation\n",
        "# SVM algorithm\n",
        "clf = SVC(random_state= 0,probability=True)\n",
        "# Fit the Algorithm\n",
        "# Predict on the model\n",
        "# Making predictions on train and test data\n",
        "analyse_model(clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "s8yyYJ6hkfQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "GRAjRCwAlsaK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DBT0n59il1uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "KAbxGvermicX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "# HYperparameter Grid\n",
        "grid = {'kernel': [\"linear\",\"rbf\",\"poly\",\"sigmoid\"],\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'max_iter' : [100,1000]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "svc = GridSearchCV(clf, param_grid = grid, scoring = scoring, cv=5,verbose=2)\n",
        "svc.fit(X_smote, y_smote)\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(svc.best_estimator_, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "O7evj5OymHZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model 5"
      ],
      "metadata": {
        "id": "NPoBTPiIBfDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_clf = KNeighborsClassifier()\n",
        "analyse_model(knn_clf, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "S_jSyYGVBg58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "JJnV-cD1C0C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HYperparameter Grid\n",
        "grid = {'n_neighbors' : [5,7,9],\n",
        "        'metric' : ['minkowski','euclidean','manhattan']}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "knn = GridSearchCV(knn_clf, param_grid = grid, scoring = scoring, cv=5,verbose=1)\n",
        "knn.fit(X_smote, y_smote)\n",
        "\n",
        "# Analysing the model with best set of parametes\n",
        "analyse_model(knn.best_estimator_, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "QwtwwPosCmL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting Naive Bayes Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nbc = GaussianNB()\n",
        "analyse_model(nbc, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "8w7odD9bDB53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-5KNtReSDW1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write the conclusion here."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}